{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import os\n",
    "import numpy as np\n",
    "from permetrics import RegressionMetric\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import netCDF4 as nc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory\n",
    "\n",
    "\n",
    "def find_file_paths(base_dir, location):\n",
    "    \"\"\"\n",
    "    Finds the correct file path for a given location based on the pattern.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): The base directory to search.\n",
    "        location (str): The location code (e.g., \"AT-Neu\").\n",
    "\n",
    "    Returns:\n",
    "        str: The path of the matching file, or None if not found.\n",
    "    \"\"\"\n",
    "    pattern = f\"FLX_{location}_FLUXNET2015_FULLSET*/FLX_{location}_FLUXNET2015_FULLSET_HH*.csv\"\n",
    "    search_pattern = os.path.join(base_dir, pattern)\n",
    "    matches = glob.glob(search_pattern)\n",
    "    if matches:\n",
    "        return matches[0]  # Return the first match (or handle multiple matches if needed)\n",
    "    else:\n",
    "        print(f\"No file found for location: {location}\")\n",
    "        return None\n",
    "\n",
    "def read_FLUXNET_site(start_date,end_date,location,base_dir,var_flx):\n",
    "    # Convert input dates to datetime objects with UTC timezone\n",
    "    start_date_obj = start_date.replace(tzinfo=pytz.UTC)\n",
    "    end_date_obj = end_date.replace(tzinfo=pytz.UTC)\n",
    "\n",
    "   # Example: Find the file paths for all locations\n",
    "    file_path= find_file_paths(base_dir, location)\n",
    "\n",
    "    # Read CSV and create a DataFrame\n",
    "    df_FLX_site = pd.read_csv(file_path, sep=',')\n",
    "\n",
    "    # Drop the second row (index 0 is the header, index 1 contains units)\n",
    "    df_FLX_site = df_FLX_site.drop(index=0)\n",
    "\n",
    "    # Parse 'TIMESTAMP_START' from the raw format\n",
    "    df_FLX_site['TIMESTAMP_START'] = pd.to_datetime(df_FLX_site['TIMESTAMP_START'], format=\"%Y%m%d%H%M\", errors='coerce')\n",
    "\n",
    "    # Check for parsing errors\n",
    "    if df_FLX_site['TIMESTAMP_START'].isna().any():\n",
    "        print(\"Some 'TIMESTAMP_START' values could not be parsed.\")\n",
    "        print(df_FLX_site[df_FLX_site['TIMESTAMP_START'].isna()])\n",
    "\n",
    "    # Convert to UTC timezone (assume 'Europe/Berlin' for initial localization)\n",
    "    # df_FLX_site['TIMESTAMP_START'] = df_FLX_site['TIMESTAMP_START'].dt.tz_localize('Europe/Berlin', nonexistent='shift_forward', ambiguous='NaT')\n",
    "    # df_FLX_site['TIMESTAMP_START'] = df_FLX_site['TIMESTAMP_START'].dt.tz_convert('UTC') # TODO this is not working, adjusting manually\n",
    "    df_FLX_site['TIMESTAMP_START'] = df_FLX_site['TIMESTAMP_START'] + pd.Timedelta(hours=0)\n",
    "    df_FLX_site['TIMESTAMP_START'] = df_FLX_site['TIMESTAMP_START'].dt.tz_localize('UTC')\n",
    "\n",
    "    # Filter the DataFrame based on the date range\n",
    "    df_FLX_site = df_FLX_site[\n",
    "        (df_FLX_site['TIMESTAMP_START'] >= start_date_obj) &\n",
    "        (df_FLX_site['TIMESTAMP_START'] <= end_date_obj)\n",
    "    ]\n",
    "\n",
    "    # Debugging: Check the filtered data\n",
    "    print(\"Filtered DataFrame shape:\", df_FLX_site.shape)\n",
    "\n",
    "    if df_FLX_site.empty:\n",
    "        print(\"No data in the specified date range.\")\n",
    "\n",
    "    # Select relevant columns and clean\n",
    "    df_FLX_site = df_FLX_site[['TIMESTAMP_START', var_flx,'NIGHT']].copy()\n",
    "    df_FLX_site = df_FLX_site.mask(df_FLX_site == -9999, np.nan)\n",
    "\n",
    "    # Ensure GPP_NT_VUT_REF is negative\n",
    "    if var_flx.startswith(\"GPP\"):\n",
    "        df_FLX_site[var_flx] = df_FLX_site[var_flx]\n",
    "        df_FLX_site[var_flx] = np.where(df_FLX_site['NIGHT'] == 1, 0, df_FLX_site[var_flx])\n",
    "        df_FLX_site[var_flx] = df_FLX_site[var_flx].clip(lower=0)\n",
    "\n",
    "    # Ensure TIMESTAMP_START is a datetime object\n",
    "    df_FLX_site['TIMESTAMP_START'] = pd.to_datetime(df_FLX_site['TIMESTAMP_START'])\n",
    "\n",
    "    # Set TIMESTAMP_START as the index\n",
    "    df_FLX_site.set_index('TIMESTAMP_START', inplace=True)\n",
    "\n",
    "    # Resample to 1-hour intervals and aggregate using the mean\n",
    "    df_FLX_site_resampled = df_FLX_site.resample('1h').mean()\n",
    "\n",
    "    # Reset the index if needed\n",
    "    df_FLX_site_resampled.reset_index(inplace=True)\n",
    "    return df_FLX_site_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the directory containing the CSV files\n",
    "csv_dir = \"/scratch/c7071034/DATA/WRFOUT/csv\"\n",
    "base_dir_FLX = \"/scratch/c7071034/DATA/Fluxnet2015/Alps\"\n",
    "fluxtypes = [\"GEE\",\"T2\",\"NEE\",\"RES\"]\n",
    "method = \"interpolated\"\n",
    "# List all relevant CSV files\n",
    "csv_files = glob.glob(f\"{csv_dir}/wrf_FLUXNET_sites_{method}_*2012-07-01 01:00:00_2012-07-31 00:00:00.csv\")\n",
    "csv_files_sorted = sorted(\n",
    "    csv_files,\n",
    "    key=lambda x: int(x.split(f'_{method}_')[1].split('km')[0]),  # Extract distance as an integer\n",
    "    reverse=True  # Sort in descending order\n",
    ")\n",
    "\n",
    "# Initialize a dictionary to store dataframes for each resolution\n",
    "dataframes = {}\n",
    "# Read each CSV into a DataFrame and store in the dictionary\n",
    "for csv_file in csv_files_sorted:\n",
    "    resolution = csv_file.split(\"_\")[-3]  # Extract resolution from filename\n",
    "    df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
    "    dataframes[resolution] = df\n",
    "\n",
    "start_date = df.index[0]\n",
    "end_date = df.index[-1]\n",
    "# Example: Plotting GEE (Gross Ecosystem Exchange) for all resolutions and locations\n",
    "locations = [ \"IT-Ren\",\"CH-Dav\", \"CH-Oe2\", \"DE-Lkb\", \"IT-Lav\", \"AT-Neu\", \"IT-MBo\", \"IT-Tor\", \"CH-Lae\"] \n",
    "# locations = [\"IT-Ren\"] \n",
    "\n",
    "locations_hgt = {\n",
    "    \"CH-Dav\": 1639, \n",
    "    \"CH-Oe2\": 452, \n",
    "    \"DE-Lkb\": 1308, \n",
    "    \"IT-Lav\": 1353, \n",
    "    \"IT-Ren\": 1730, \n",
    "    \"AT-Neu\": 970, \n",
    "    \"IT-MBo\": 1550, \n",
    "    \"IT-Tor\": 2160, \n",
    "    \"CH-Lae\": 689,\n",
    "    }\n",
    "\n",
    "\n",
    "resolution_colors = {\n",
    "    \"3km\": \"blue\",\n",
    "    \"9km\": \"purple\",\n",
    "    \"27km\": \"red\",\n",
    "    \"54km\": \"green\",\n",
    "}\n",
    "\n",
    "    # Define target locations (latitude, longitude)\n",
    "locations_ll = [\n",
    "        {\"name\": \"CH-Oe2\", \"lat\": 47.2863, \"lon\": 7.7343},\n",
    "        {\"name\": \"CH-Dav\", \"lat\": 46.8153, \"lon\": 9.8559},\n",
    "        {\"name\": \"DE-Lkb\", \"lat\": 49.0996, \"lon\": 13.3047},\n",
    "        {\"name\": \"IT-Lav\", \"lat\": 45.9562, \"lon\": 11.2813},\n",
    "        {\"name\": \"IT-Ren\", \"lat\": 46.5869, \"lon\": 11.4337},\n",
    "        {\"name\": \"AT-Neu\", \"lat\": 47.1167, \"lon\": 11.3175},\n",
    "        {\"name\": \"IT-MBo\", \"lat\": 46.0147, \"lon\": 11.0458},\n",
    "        {\"name\": \"IT-Tor\", \"lat\": 45.8444, \"lon\": 7.5781},\n",
    "        {\"name\": \"CH-Lae\", \"lat\": 47.4781, \"lon\": 8.3644},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CAMS data\n",
    "CAMS_data_dir_path = \"/scratch/c7071034/DATA/CAMS/\"\n",
    "\n",
    "path_CAMS_file = os.path.join(\n",
    "    CAMS_data_dir_path + \"ghg-reanalysis_surface_2012-07-01_2012-07-31.nc\" # check this!!\n",
    ")\n",
    "CAMS_data = nc.Dataset(path_CAMS_file)\n",
    "times_CAMS = CAMS_data.variables[\"valid_time\"]\n",
    "\n",
    "CAMS_vars = [\"fco2nee\", \"fco2gpp\", \"fco2rec\", \"t2m\"]\n",
    "factor_kgC = 1000000 / 0.04401  # conversion from kgCO2/m2/s to mgC/m2/s to  mumol/m2/s\n",
    "factors = [-factor_kgC, factor_kgC, -factor_kgC, 1]\n",
    "\n",
    "lat_CAMS = CAMS_data.variables[\"latitude\"][:]\n",
    "lon_CAMS = CAMS_data.variables[\"longitude\"][:]\n",
    "\n",
    "def get_int_var(lat_target, lon_target, lats, lons, var_CAMS):\n",
    "    interpolator = RegularGridInterpolator((lats, lons), var_CAMS)\n",
    "    interpolated_value = interpolator((lat_target, lon_target))\n",
    "    return interpolated_value\n",
    "\n",
    "df_CAMS_hourly_all = {}\n",
    "for location_ll in locations_ll:\n",
    "    lat_target, lon_target = location_ll[\"lat\"], location_ll[\"lon\"]\n",
    "\n",
    "    data_rows = []\n",
    "    new_time = start_date - timedelta(minutes=60)\n",
    "    ts_CAMS = pd.DataFrame()\n",
    "    j = 0\n",
    "    for time_CAMS in times_CAMS:\n",
    "        date_CAMS = datetime(1970, 1, 1) + timedelta(seconds=int(time_CAMS))\n",
    "        formatted_time = date_CAMS.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        row = {'time': formatted_time}\n",
    "        for CAMS_var, factor in zip(CAMS_vars, factors):\n",
    "            var_CAMS = (\n",
    "                CAMS_data.variables[CAMS_var][j, :, :].data * factor\n",
    "            )  # convert unit to mmol m-2 s-1\n",
    "            row[CAMS_var] = get_int_var(lat_target, lon_target, lat_CAMS, lon_CAMS, var_CAMS)\n",
    "\n",
    "        j += 1\n",
    "        data_rows.append(row)\n",
    "    \n",
    "    df_CAMS = pd.DataFrame(data_rows)\n",
    "    df_CAMS['time'] = pd.to_datetime(df_CAMS['time'])\n",
    "    df_CAMS.set_index('time', inplace=True)\n",
    "    df_CAMS = df_CAMS.astype(float)\n",
    "    df_CAMS_hourly = df_CAMS.resample('h').interpolate(method='linear')\n",
    "    df_CAMS_hourly[\"fco2gpp\"] = df_CAMS_hourly[\"fco2gpp\"]\n",
    "    df_CAMS_hourly[\"t2m\"] -= 273.15\n",
    "    \n",
    "    # Store the DataFrame in the dictionary\n",
    "    df_CAMS_hourly_all[location_ll[\"name\"]] = df_CAMS_hourly\n",
    "\n",
    "# for location_name, df in df_CAMS_hourly_all.items():\n",
    "#     print(f\"Data for location: {location_name}\")\n",
    "#     print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create plots for each location\n",
    "consolidated_metrics_all = pd.DataFrame()\n",
    "for location in locations:\n",
    "    for fluxtype in fluxtypes:\n",
    "        consolidated_metrics_df = pd.DataFrame(\n",
    "            index=[\"RMSE\", \"MAE\", \"MAPE\", \"R2\", \"NNSE\", \"KGE\"],  # Metrics as rows\n",
    "        )\n",
    "        variables = {f\"ref_{fluxtype}\": \"dotted\", f\"std_{fluxtype}\": \"dashed\", f\"tune_{fluxtype}\": \"solid\"}  # {\"ref_GEE\": \"solid\", \"std_GEE\": \"dashed\", \"tune_GEE\": \"dotted\"} \n",
    "        if fluxtype == \"RES\":\n",
    "            var_flx = \"RECO_NT_VUT_USTAR50\"            \n",
    "            df_FLX_site = read_FLUXNET_site(start_date,end_date,location,base_dir_FLX,var_flx)\n",
    "            var_CAMS_plot = \"fco2rec\"\n",
    "        elif fluxtype == \"GEE\":\n",
    "            var_flx = \"GPP_NT_VUT_USTAR50\"\n",
    "            df_FLX_site = read_FLUXNET_site(start_date,end_date,location,base_dir_FLX,var_flx)\n",
    "            var_CAMS_plot = \"fco2gpp\"\n",
    "        elif fluxtype == \"NEE\":\n",
    "            var_flx = \"NEE_VUT_USTAR50\"\n",
    "            df_FLX_site = read_FLUXNET_site(start_date,end_date,location,base_dir_FLX,var_flx)\n",
    "            var_CAMS_plot = \"fco2nee\"\n",
    "        elif fluxtype == \"T2\":\n",
    "            var_flx = \"TA_F\"\n",
    "            df_FLX_site = read_FLUXNET_site(start_date,end_date,location,base_dir_FLX,var_flx)\n",
    "            var_CAMS_plot = \"t2m\"\n",
    "\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        for resolution, df in dataframes.items():\n",
    "            # Subset the DataFrame to columns relevant to this location and variables\n",
    "            if fluxtype == \"NEE\":\n",
    "                df_loc_gee = df.filter(regex=f\"^{location}_(.*_GEE)$\") \n",
    "                df_loc_res = df.filter(regex=f\"^{location}_(.*_RES)$\") \n",
    "                df_loc = df_loc_res.add(df_loc_gee, fill_value=0)\n",
    "                nee_columns = {} # Dynamically create NEE columns by summing corresponding GEE and RES columns\n",
    "                for col in df_loc.columns:\n",
    "                    if '_GEE' in col:\n",
    "                        prefix = col.replace('_GEE', '')\n",
    "                        if f'{prefix}_RES' in df_loc.columns:\n",
    "                            nee_columns[f'{prefix}_NEE'] = (f'{prefix}_RES', f'{prefix}_GEE')\n",
    "                for nee_col, (res_col, gee_col) in nee_columns.items():\n",
    "                    df_loc[nee_col] = df_loc[res_col] + df_loc[gee_col]\n",
    "                df_loc = df_loc.drop(columns=df_loc.filter(regex=\"(_GEE|_RES)$\").columns)\n",
    "            elif fluxtype == \"T2\":\n",
    "                df_loc = df.filter(regex=f\"^{location}_(.*_{fluxtype})$\")  # Match location and variable pattern\n",
    "                df_loc = df_loc - 273.15\n",
    "            elif fluxtype == \"GEE\":\n",
    "                df_loc = -df.filter(regex=f\"^{location}_(.*_{fluxtype})$\")  # turning GEE to positive values\n",
    "            elif fluxtype == \"RES\": \n",
    "                df_loc = df.filter(regex=f\"^{location}_(.*_{fluxtype})$\")  \n",
    "                \n",
    "            site_match_all = pd.read_csv(f\"pft_site_match_at_{resolution}.csv\")\n",
    "            site_match = site_match_all[site_match_all[\"site\"] == location]\n",
    "\n",
    "            for column in df_loc.columns:\n",
    "                variable = column.split(\"_\")[-2] +\"_\"+ column.split(\"_\")[-1]  # Extract the variable type (e.g., ref_GEE, std_GEE)\n",
    "                if variable in variables:\n",
    "                    evaluator = RegressionMetric(\n",
    "                        df_FLX_site[var_flx].tolist(),\n",
    "                        df_loc[column].tolist(),\n",
    "                    )\n",
    "                    \n",
    "                    metrics = evaluator.get_metrics_by_list_names([\"RMSE\", \"MAE\", \"MAPE\", \"R2\", \"NNSE\", \"KGE\"]) \n",
    "                    # save metric data for csv\n",
    "                    for metric, value in metrics.items():\n",
    "                        consolidated_metrics_df.loc[metric, column+\"_\"+resolution] = value  # Assign metric value to the appropriate column\n",
    "\n",
    "                    linestyle = variables[variable]\n",
    "                    if site_match[\"pft_match\"].any():\n",
    "                        color_i = resolution_colors[resolution]\n",
    "                        alpha_i = 0.7\n",
    "                    else: \n",
    "                        color_i = \"grey\"\n",
    "                        alpha_i = 0.1\n",
    "\n",
    "                    plt.plot(\n",
    "                        df_loc.index,  # Skipping the first index as requested\n",
    "                        df_loc[column],  # Skipping the first value as requested\n",
    "                       label = (f\"{resolution}_{variable.split('_')[0]} R2={metrics['R2']:.2f} at {site_match['model_pft_VPRM'].values[0]}, {site_match['model_pft_CLC'].values[0]}, hgt={int(site_match[f'model_hgt_{method}'].iloc[0])} \"),\n",
    "                        linestyle=linestyle,\n",
    "                        color=color_i,\n",
    "                        alpha=alpha_i, \n",
    "                    )\n",
    "\n",
    "        if location in df_CAMS_hourly_all:\n",
    "            df_CAMS_hourly_l = df_CAMS_hourly_all[location]\n",
    "        cams_l = df_CAMS_hourly_l[var_CAMS_plot][1:len(df_FLX_site[var_flx])+1]\n",
    "        evaluator = RegressionMetric(\n",
    "            df_FLX_site[var_flx].tolist(),\n",
    "            cams_l.tolist(),\n",
    "        )\n",
    "        \n",
    "        metrics_cams = evaluator.get_metrics_by_list_names([\"RMSE\", \"MAE\", \"MAPE\", \"R2\", \"NNSE\", \"KGE\"]) \n",
    "        plt.plot(\n",
    "                df_CAMS_hourly.index[1:len(df_FLX_site[var_flx])+1],  \n",
    "                cams_l,  \n",
    "                label=f\"CAMS {var_CAMS_plot} - R2={metrics_cams['R2']:.2f} \",\n",
    "                linestyle=\"dashed\",\n",
    "                color=\"orange\",\n",
    "        )\n",
    "        plt.plot(\n",
    "                df_loc.index,  \n",
    "                df_FLX_site[var_flx], \n",
    "                label=f\"FLUXNET {var_flx}\",\n",
    "                linestyle=\"solid\",\n",
    "                color=\"black\",\n",
    "        )\n",
    "        max_value = consolidated_metrics_df.loc[\"R2\", :].max()\n",
    "        max_column = consolidated_metrics_df.loc[\"R2\", :].idxmax()\n",
    "        plt.title(f\"{var_flx}, CAMS and WRF {method} at {location} - {locations_hgt[location]}m ASL \\n best R2={max_value:0.2f} - {max_column.split(\"_\")[1]} at {max_column.split(\"_\")[3]})\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(var_flx)\n",
    "        plt.legend(title=\"Resolution and Type\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{csv_dir}/{location}_{fluxtype}_comparison_{method}.png\")  # Save plot as PNG\n",
    "        plt.show()\n",
    "        consolidated_metrics_all = pd.concat([consolidated_metrics_all,consolidated_metrics_df.T])\n",
    "\n",
    "consolidated_metrics_all.to_csv(f\"{csv_dir}/consolidated_metrics_{method}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create plots for each location\n",
    "consolidated_metrics_all = pd.DataFrame()\n",
    "for location in locations:\n",
    "    for fluxtype in fluxtypes:\n",
    "        consolidated_metrics_df = pd.DataFrame(\n",
    "            index=[\"RMSE\", \"MAE\", \"MAPE\", \"R2\", \"NNSE\", \"KGE\"],  # Metrics as rows\n",
    "        )\n",
    "        variables = {f\"ref_{fluxtype}\": \"dotted\", f\"std_{fluxtype}\": \"dashed\", f\"tune_{fluxtype}\": \"solid\"}  # {\"ref_GEE\": \"solid\", \"std_GEE\": \"dashed\", \"tune_GEE\": \"dotted\"} \n",
    "        if fluxtype == \"RES\":\n",
    "            var_flx = \"RECO_NT_VUT_USTAR50\"            \n",
    "            df_FLX_site = read_FLUXNET_site(start_date,end_date,location,base_dir_FLX,var_flx)\n",
    "            var_CAMS_plot = \"fco2rec\"\n",
    "        elif fluxtype == \"GEE\":\n",
    "            var_flx = \"GPP_NT_VUT_USTAR50\"\n",
    "            df_FLX_site = read_FLUXNET_site(start_date,end_date,location,base_dir_FLX,var_flx)\n",
    "            var_CAMS_plot = \"fco2gpp\"\n",
    "        elif fluxtype == \"NEE\":\n",
    "            var_flx = \"NEE_VUT_USTAR50\"\n",
    "            df_FLX_site = read_FLUXNET_site(start_date,end_date,location,base_dir_FLX,var_flx)\n",
    "            var_CAMS_plot = \"fco2nee\"\n",
    "        elif fluxtype == \"T2\":\n",
    "            var_flx = \"TA_F\"\n",
    "            df_FLX_site = read_FLUXNET_site(start_date,end_date,location,base_dir_FLX,var_flx)\n",
    "            var_CAMS_plot = \"t2m\"\n",
    "\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        for resolution, df in dataframes.items():\n",
    "            # Subset the DataFrame to columns relevant to this location and variables\n",
    "            if fluxtype == \"NEE\":\n",
    "                df_loc_gee = df.filter(regex=f\"^{location}_(.*_GEE)$\") \n",
    "                df_loc_res = df.filter(regex=f\"^{location}_(.*_RES)$\") \n",
    "                df_loc = df_loc_res.add(df_loc_gee, fill_value=0)\n",
    "                nee_columns = {} # Dynamically create NEE columns by summing corresponding GEE and RES columns\n",
    "                for col in df_loc.columns:\n",
    "                    if '_GEE' in col:\n",
    "                        prefix = col.replace('_GEE', '')\n",
    "                        if f'{prefix}_RES' in df_loc.columns:\n",
    "                            nee_columns[f'{prefix}_NEE'] = (f'{prefix}_RES', f'{prefix}_GEE')\n",
    "                for nee_col, (res_col, gee_col) in nee_columns.items():\n",
    "                    df_loc[nee_col] = df_loc[res_col] + df_loc[gee_col]\n",
    "                df_loc = df_loc.drop(columns=df_loc.filter(regex=\"(_GEE|_RES)$\").columns)\n",
    "            elif fluxtype == \"T2\":\n",
    "                df_loc = df.filter(regex=f\"^{location}_(.*_{fluxtype})$\")  # Match location and variable pattern\n",
    "                df_loc = df_loc - 273.15\n",
    "            elif fluxtype == \"GEE\":\n",
    "                df_loc = -df.filter(regex=f\"^{location}_(.*_{fluxtype})$\")  # turning GEE to positive values\n",
    "            elif fluxtype == \"RES\": \n",
    "                df_loc = df.filter(regex=f\"^{location}_(.*_{fluxtype})$\")  \n",
    "                \n",
    "            site_match_all = pd.read_csv(f\"pft_site_match_at_{resolution}.csv\")\n",
    "            site_match = site_match_all[site_match_all[\"site\"] == location]\n",
    "\n",
    "            for column in df_loc.columns:\n",
    "                variable = column.split(\"_\")[-2] +\"_\"+ column.split(\"_\")[-1]  # Extract the variable type (e.g., ref_GEE, std_GEE)\n",
    "                if variable in variables:\n",
    "                    df_loc[\"hour\"] = df_loc.index.hour\n",
    "                    hourly_avg = df_loc.groupby(\"hour\")[column].mean()\n",
    "\n",
    "                    linestyle = variables[variable]\n",
    "                    if site_match[\"pft_match\"].any():\n",
    "                        color_i = resolution_colors[resolution]\n",
    "                        alpha_i = 0.7\n",
    "                    else: \n",
    "                        color_i = \"grey\"\n",
    "                        alpha_i = 0.1\n",
    "\n",
    "                    plt.plot(\n",
    "                        hourly_avg,  # Skipping the first value as requested\n",
    "                       label = (f\"{resolution}_{variable.split('_')[0]} at hgt={int(site_match[f'model_hgt_{method}'].iloc[0])} with {site_match['model_pft_VPRM'].values[0]}, \"),\n",
    "                        linestyle=linestyle,\n",
    "                        color=color_i,\n",
    "                        alpha=alpha_i, \n",
    "                    )\n",
    "\n",
    "        if location in df_CAMS_hourly_all:\n",
    "            df_CAMS_hourly_l = df_CAMS_hourly_all[location]\n",
    "        cams_l = df_CAMS_hourly_l[var_CAMS_plot][1:len(df_FLX_site[var_flx])+1]\n",
    "\n",
    "        df_CAMS_hourly[\"hour\"] = df_CAMS_hourly.index.hour\n",
    "        hourly_avg_CAMS = df_CAMS_hourly.groupby(\"hour\")[var_CAMS_plot].mean()\n",
    "\n",
    "        plt.plot(\n",
    "                hourly_avg_CAMS,  \n",
    "                label=f\"CAMS {var_CAMS_plot} \",\n",
    "                linestyle=\"dashed\",\n",
    "                color=\"orange\",\n",
    "        )\n",
    "\n",
    "        df_FLX_site[\"TIMESTAMP_START\"] = pd.to_datetime(df_FLX_site[\"TIMESTAMP_START\"])\n",
    "        df_FLX_site.set_index('TIMESTAMP_START', inplace=True)\n",
    "        df_FLX_site[\"hour\"] = df_FLX_site.index.hour\n",
    "        hourly_avg_FLX = df_FLX_site.groupby(\"hour\")[var_flx].mean()\n",
    "        plt.plot(\n",
    "                hourly_avg_FLX, \n",
    "                label=f\"FLUXNET {var_flx}\",\n",
    "                linestyle=\"solid\",\n",
    "                color=\"black\",\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{var_flx}, CAMS and WRF {method} at {location} - {locations_hgt[location]}m ASL \\n {max_column.split(\"_\")[1]} at {max_column.split(\"_\")[3]})\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(var_flx + \" hourly average\")\n",
    "        plt.legend(title=\"Resolution and Type\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{csv_dir}/{location}_{fluxtype}_comparison_{method}_hourly_means.png\")  # Save plot as PNG\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
